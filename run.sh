PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32 CUDA_VISIBLE_DEVICES=0 python run_ds_transformer.py -ep=25 -t=ds_mono12 -lr 0.01 -bs 32 -lt=triplet_mmd -dc 0.9 -stop 26 -seed 333 -a 1.0 -tm 1 -p 0.9 -q 0.1 -qm 1.5 -r 1.0
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32 CUDA_VISIBLE_DEVICES=0 python run_ds_transformer.py -ep=25 -t=ds_mono14 -lr 0.01 -bs 32 -lt=triplet_mmd -dc 0.9 -stop 26 -seed 333 -a 1.0 -tm 1 -p 0.9 -q 0.1 -qm 1.5 -r 1.0
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32 CUDA_VISIBLE_DEVICES=0 python run_ds_transformer.py -ep=25 -t=ds_mono15 -lr 0.01 -bs 32 -lt=triplet_mmd -dc 0.9 -stop 26 -seed 333 -a 1.0 -tm 1 -p 0.9 -q 0.1 -qm 1.5 -r 1.0
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32 CUDA_VISIBLE_DEVICES=0 python run_ds_transformer.py -ep=25 -t=ds_mono16 -lr 0.01 -bs 32 -lt=triplet_mmd -dc 0.9 -stop 26 -seed 333 -a 1.0 -tm 1 -p 0.9 -q 0.1 -qm 1.5 -r 1.0

PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32 CUDA_VISIBLE_DEVICES=0 python run_ds_transformer.py -ep=25 -t=ds_mono17 -lr 0.01 -bs 32 -lt=triplet_mmd -dc 0.9 -stop 26 -seed 333 -a 1.0 -tm 1 -p 0.9 -q 0.1 -qm 1.5 -r 1.0
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32 CUDA_VISIBLE_DEVICES=0 python run_ds_transformer.py -ep=25 -t=ds_mono18 -lr 0.01 -bs 32 -lt=triplet_mmd -dc 0.9 -stop 26 -seed 333 -a 1.0 -tm 1 -p 0.9 -q 0.1 -qm 1.5 -r 1.0
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32 CUDA_VISIBLE_DEVICES=0 python run_ds_transformer.py -ep=25 -t=ds_mono19 -lr 0.01 -bs 32 -lt=triplet_mmd -dc 0.9 -stop 26 -seed 333 -a 1.0 -tm 1 -p 0.9 -q 0.1 -qm 1.5 -r 1.0
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:32 CUDA_VISIBLE_DEVICES=0 python run_ds_transformer.py -ep=25 -t=ds_mono20 -lr 0.01 -bs 32 -lt=triplet_mmd -dc 0.9 -stop 26 -seed 333 -a 1.0 -tm 1 -p 0.9 -q 0.1 -qm 1.5 -r 1.0